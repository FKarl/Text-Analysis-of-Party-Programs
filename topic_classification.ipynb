{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Topic classification using cosine similarity on the word embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "Same as in the data_exploration notebook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "import re\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "class Party(Enum):\n",
    "    AFD = 0\n",
    "    CDU = 1\n",
    "    FDP = 2\n",
    "    GRUENE = 3\n",
    "    LINKE = 4\n",
    "    SPD = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "# stopwords\n",
    "nltk_stopwords = stopwords.words('german')\n",
    "\n",
    "# build stopwords list\n",
    "all_stopwords = list(set(STOP_WORDS) | set(nltk_stopwords))\n",
    "with open('custom_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    all_stopwords += [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load files\n",
    "party_text = {}\n",
    "for party in Party:\n",
    "    all_stopwords.extend(['{}'.format(party.name.lower())])\n",
    "    with open('resources/' + party.name + '.txt', encoding='utf-8', errors='ignore') as txt:\n",
    "        file = \" \".join(l for l in txt)\n",
    "        # remove gender *\n",
    "        file = re.sub(r'\\*innen(\\w*)\\s', r'\\1 ', file)\n",
    "    party_text[party] = file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    doc = nlp(\" \".join(texts))\n",
    "    texts_out = [token.lemma_ for token in doc if token.pos_ in allowed_postags and token.lemma_ not in all_stopwords]\n",
    "\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def prepare_data(party: Party):\n",
    "    \"\"\"\n",
    "    Prepare data for topic classification.\n",
    "    Split into sections and lemmatize.\n",
    "\n",
    "    :param party: The party to get the text from\n",
    "    :return: a dict containing the text and the lemmatized text seperated in sections\n",
    "    \"\"\"\n",
    "    # get sections\n",
    "    sections = re.split(r'\\n\\s\\n', party_text[party])\n",
    "    cleaned_sections = {}\n",
    "    for section in sections:\n",
    "        wordbag = gensim.utils.simple_preprocess(section)\n",
    "        #lemmatize\n",
    "        # wordbag = lemmatization(wordbag)\n",
    "        cleaned_sections[section] = wordbag\n",
    "\n",
    "    # return all cleaned sections except empty ones\n",
    "    return dict(filter(lambda x: len(x[1]) > 0, cleaned_sections.items()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create target clusters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from transformers import BertTokenizer, TFBertModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "# based on lda from the data_exploration notebook\n",
    "cluster_dict = {\n",
    "    \"UMWELT\":\n",
    "        [\"umwelt\", \"klima\", \"klimaschutz\", \"ökologie\", \"co2\", \"co\", \"landwirtschaft\", \"klimakrise\", \"treibhaus\",\n",
    "         \"emissionen\", \"ausbau\", \"innovative\", \"natur\", \"wasser\", \"landwirtschaft\", \"nachhaltig\", \"strom\",\n",
    "         \"solar\", \"windkraft\", \"wasserkraft\", \"kohlekraft\", \"atomkraft\", \"kernenergie\", \"energiewende\",\n",
    "         \"erneuerbar\", \"öl\", \"gas\", \"energie\", \"nachhaltig\", \"wasserstoff\", \"luftqualität\", \"umwelt\", \"wälder\",\n",
    "         \"erneuerbare energie\"],\n",
    "\n",
    "    \"WIRTSCHAFT\":\n",
    "        [\"unternehmen\", \"wettbewerb\", \"digitale\", \"selbstständige\", \"firma\", \"markt\",\n",
    "         \"kapital\", \"finanzierung\", \"ezb\", \"banken\", \"staatsanleihen\", \"währungsfonds\",\n",
    "         \"staatsanleihen\", \"kredit\", \"investitionen\", \"euro\", \"industrie\", \"schulden\", \"steuern\",\n",
    "         \"konzern\", \"kapital\", \"finanzen\"],\n",
    "\n",
    "    \"BILDUNG\":\n",
    "        [\"schule\", \"gymnasium\", \"realschule\", \"grundschule\", \"mittelschule\", \"lehrer\", \"universität\", \"lehre\",\n",
    "         \"gesamtschule\", \"bildung\", \"hochschulen\", \"wissenschaft\", \"bildung\", \"erasmus\", \"forschung\",\n",
    "         \"lehre\", \"entwickeln\", \"weiterbildung\", \"aufstiegsmöglichkeit\", \"bildungsstandard\",\"innovationen\"],\n",
    "\n",
    "    \"GESELLSCHAFT\":\n",
    "        [\"gesellschaft\", \"kultur\", \"freiheit\", \"privat\", \"kulturelle\", \"antisemitismus\",\n",
    "         \"vorbild\", \"frauen\", \"familie\", \"identität\", \"gender\", \"sprache\", \"leben\", \"religion\", \"christentum\", \"islam\", \"diskriminierung\",\n",
    "         \"menschenrechte\", \"kunst\", \"adoption\"],\n",
    "\n",
    "    \"INNEN\":\n",
    "        [\"schutz\", \"polizei\", \"schützen\", \"überwachung\", \"datenschutz\", \"sicherheit\", \"bundeswehr\", \"asyl\",\n",
    "         \"integration\", \"migrant\", \"flüchtling\", \"immigrant\", \"toleranz\", \"zuwanderung\", \"asylbewerber\", \"innere\",\n",
    "         \"krimminalität\", \"zuwanderung\", \"kontrolle\", \"bundespolizei\", \"rechtsstaat\", \"gefahr\", \"terroristen\",\n",
    "         \"gewalt\"],\n",
    "\n",
    "    \"ARBEIT_UND_SOZIALES\":\n",
    "        [\"rente\", \"harz4\", \"arbeitslosengeld\", \"pflege\", \"wohngeld\", \"familie\", \"arbeitslos\", \"sozial\", \"bauen\",\n",
    "         \"wohnungen\", \"sozialbau\", \"kinder\", \"pflegen\", \"arbeitssuchende\", \"grundsicherung\", \"eltern\", \"jugendlich\",\n",
    "         \"gesundheit\", \"arzt\", \"armut\", \"einkommen\", \"löhne\", \"tarifvertrag\", \"bürgergeld\",\n",
    "         \"sozialstaat\", \"teilhabe\", \"bezahlbar\"],\n",
    "\n",
    "    # default:\n",
    "    #\"KEIN_THEMA\": []\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dbmdz/bert-base-german-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at dbmdz/bert-base-german-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-uncased\", do_lower_case=True)\n",
    "model = TFBertModel.from_pretrained(\"dbmdz/bert-base-german-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "def convert_to_embeddings(cluster):\n",
    "    idx = tokenizer.encode(cluster)\n",
    "    idx = np.array(idx)[None, :]\n",
    "    embedding = model(idx)\n",
    "    tensor = np.array(embedding[0][0][1:-1])\n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "# create list of vectors\n",
    "section_dict = prepare_data(Party.AFD)\n",
    "mean_vectors = [convert_to_embeddings(txt).mean(0) for txt in section_dict.values()]\n",
    "\n",
    "feature_matrix = np.array(mean_vectors)\n",
    "\n",
    "# vector space for dict (maybe mean(0) ?)\n",
    "dic_vectors = {key: convert_to_embeddings(value) for key, value in cluster_dict.items()}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "similarities = np.array(\n",
    "    [metrics.pairwise.cosine_similarity(feature_matrix, dic_y).T.tolist()[0] for dic_y in dic_vectors.values()]).T\n",
    "\n",
    "labels = list(dic_vectors.keys())\n",
    "for i in range(len(similarities)):\n",
    "\n",
    "    # if there is a similarity to a cluster -> random cluster\n",
    "    if sum(similarities[i]) == 0:\n",
    "        similarities[i] = [0] * len(labels)\n",
    "        similarities[i][np.random.choice(range(len(labels)))] = 1\n",
    "\n",
    "    # rescale to 1\n",
    "    similarities[i] = similarities[i] / sum(similarities[i])\n",
    "\n",
    "#classify based on similarities\n",
    "prediction = [(labels[np.argmax(pred)], max(pred)) for pred in similarities]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMWELT : 11\n",
      "\n",
      "WIRTSCHAFT : 135\n",
      "\n",
      "BILDUNG : 2\n",
      "\n",
      "GESELLSCHAFT : 109\n",
      "\n",
      "INNEN : 166\n",
      "\n",
      "ARBEIT_UND_SOZIALES : 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for topic in labels:\n",
    "    print(topic, \":\", len(list(filter(lambda x: x[0] == topic, prediction))))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eine existentielle Frage\n",
      " wie die Zuwanderung\n",
      " muss in demokratischer\n",
      " Selbstbestimmung\n",
      " auf nationaler Ebene\n",
      " entschieden werden. \n",
      " --> ('UMWELT', 0.18235809743927947)\n",
      "\n",
      " Die Regierungspolitiker in Bund und Ländern haben mit\n",
      " ihrer Flüchtlings-, Europa- und Corona-Politik die\n",
      " Prinzipien der deutschen Staatlichkeit, des Rechts und\n",
      " der Verfassung vielfach verletzt.\n",
      " Zugleich haben sich die Volksvertreter der etablierten\n",
      " Parteien den grundgesetzlich garantierten Parlamentsvorbehalt für alle wichtigen Entscheidungen im Staat\n",
      " ohne Widerstand nehmen lassen.\n",
      " Die Bundesregierung kommt ihrer Pflicht, Vertragsbrüchen und Selbstermächtigungen durch\n",
      " EU-Institutionen entgegenzutreten, nicht nach. Einzelne\n",
      " rechts- und verfassungswidrige Maßnahmen wurden\n",
      " zwar durch mutige Richter in Hunderten von Urteilen zu\n",
      " Fall gebracht. In unserem Land hat sich aber eine\n",
      " politische Klasse herausgebildet, deren vordringliches\n",
      " Interesse ihrer Macht, ihrem Status und ihrem\n",
      " materiellen Wohlergehen gilt. Diese setzt die soziale und\n",
      " kulturelle Zukunft unseres Volkes, die Stärke unserer\n",
      " Wirtschaft und damit unseres Wohlstandes aufs Spiel\n",
      " und stellt Multikulturalität, Diversität, Globalisierung\n",
      " und vermeintliche Gendergerechtigkeit über alles.\n",
      " Sie hält die Schalthebel der staatlichen Macht, der\n",
      " politischen Bildung und des informationellen und\n",
      " medialen Einflusses auf die Bevölkerung in Händen.\n",
      " Deshalb halten wir die unmittelbare Demokratie für ein\n",
      " unverzichtbares Mittel, um dem autoritären und\n",
      " teilweise totalitären Gebaren der Regierungspolitiker\n",
      " Einhalt zu gebieten. \n",
      " --> ('WIRTSCHAFT', 0.18031960438679118)\n",
      "\n",
      " Steuern und\n",
      " Finanzen \n",
      " --> ('BILDUNG', 0.21299966196895123)\n",
      "\n",
      "\n",
      " KAPITEL 1 \n",
      " --> ('GESELLSCHAFT', 0.18925639254163473)\n",
      "\n",
      " Wir halten die unmittelbare Demokratie für ein\n",
      " unverzichtbares Mittel,\n",
      " um dem autoritären\n",
      " und teilweise totalitären\n",
      " Gebaren der Regierungspolitiker Einhalt\n",
      " zu gebieten. \n",
      " --> ('INNEN', 0.18056599662086964)\n",
      "\n",
      " Die AfD will Familien\n",
      " entlasten, indem ihnen\n",
      " Rentenbeiträge in Höhe\n",
      " von 20.000 € pro Kind\n",
      " freigestellt werden . \n",
      " --> ('ARBEIT_UND_SOZIALES', 0.17526207896375726)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#k = 4\n",
    "#print(list(section_dict.keys())[k], '\\n --> ', prediction[k])\n",
    "\n",
    "# print first occurence of each label\n",
    "for label in labels:\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i][0] == label:\n",
    "            print(list(section_dict.keys())[i], \"\\n -->\", prediction[i])\n",
    "            print()\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}